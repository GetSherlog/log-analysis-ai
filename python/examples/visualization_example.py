#!/usr/bin/env python3
"""
LogAI Data Analysis and Visualization Example

This script demonstrates how to use Python code (as would be generated by the LLM)
to analyze and visualize log data from the LogAI CLI.
"""

import os
import sys
from pathlib import Path
import json

# Add the parent directory to the Python path
sys.path.append(str(Path(__file__).parent.parent))

# Import our LogAI agent
from logai_agent import LogAIAgent

def print_section(title):
    """Print a section title."""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)

def main():
    # Initialize the LogAI agent
    agent = LogAIAgent()
    
    # Load sample logs
    sample_path = os.path.join(Path(__file__).parent.parent, "samples", "sample_logs.jsonl")
    agent.initialize(sample_path)
    print(f"Initialized agent with logs from {sample_path}")
    
    # 1. Log level distribution analysis
    print_section("LOG LEVEL DISTRIBUTION ANALYSIS")
    
    # First, get statistics about log levels
    stats = agent.tools["calculate_statistics"]()
    
    # Example of LLM-generated code for visualization
    # This would normally be generated by the LLM based on the user's question
    visualization_code = """
# Create a bar chart of log level distribution
if 'level_df' in locals():
    plt.figure(figsize=(10, 6))
    
    # Sort levels by severity
    level_order = ['TRACE', 'DEBUG', 'INFO', 'WARN', 'WARNING', 'ERROR', 'FATAL', 'CRITICAL']
    level_df['level'] = pd.Categorical(level_df['level'], categories=level_order, ordered=True)
    level_df = level_df.sort_values('level')
    
    # Create bar chart with custom colors
    ax = sns.barplot(x='level', y='count', data=level_df, 
                    palette={'INFO': 'green', 'WARN': 'orange', 'ERROR': 'red', 'DEBUG': 'blue', 
                             'TRACE': 'lightblue', 'WARNING': 'orange', 'FATAL': 'darkred', 'CRITICAL': 'purple'})
    
    # Add value labels on top of bars
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', 
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha = 'center', va = 'bottom',
                    xytext = (0, 5), textcoords = 'offset points')
    
    # Add title and labels
    plt.title('Distribution of Log Levels')
    plt.xlabel('Log Level')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Store analysis results
    results['total_logs'] = level_df['count'].sum()
    results['error_percentage'] = level_df[level_df['level'].isin(['ERROR', 'FATAL', 'CRITICAL'])]['count'].sum() / results['total_logs'] * 100
"""

    # Execute the visualization code
    viz_result = agent.tools["visualize_data"](
        data=stats,
        code=visualization_code
    )
    
    if viz_result.get("success"):
        print(f"✓ Created log level distribution chart: {viz_result['saved_path']}")
        if viz_result.get("data"):
            print(f"Analysis results: {json.dumps(viz_result['data'], indent=2)}")
    else:
        print(f"× Error creating visualization: {viz_result.get('error')}")
    
    
    # 2. Time-based trend analysis
    print_section("TIME TREND ANALYSIS")
    
    # Get trending patterns
    trends = agent.tools["get_trending_patterns"](time_window="hour")
    
    # Example of LLM-generated code for time series analysis
    trend_analysis_code = """
# Analyze trends over time
if 'trends_df' in locals():
    # Convert timestamps to datetime if needed
    if 'window' in trends_df.columns:
        try:
            trends_df['window'] = pd.to_datetime(trends_df['window'])
        except:
            pass
    
    # Create line chart
    plt.figure(figsize=(12, 6))
    
    # If there are multiple metrics, plot each one
    if 'metric' in trends_df.columns:
        for metric, group in trends_df.groupby('metric'):
            plt.plot(group['window'], group['count'], marker='o', linestyle='-', label=metric)
        plt.legend()
    else:
        # Single metric line plot
        plt.plot(trends_df['window'], trends_df['count'], marker='o', linestyle='-', color='blue')
    
    # Add labels and title
    plt.title('Log Activity Trends Over Time')
    plt.xlabel('Time Window')
    plt.ylabel('Count')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Calculate trend statistics
    results['total_events'] = trends_df['count'].sum()
    results['peak_time'] = trends_df.loc[trends_df['count'].idxmax(), 'window']
    results['peak_count'] = trends_df['count'].max()
    
    # Calculate growth rate
    if len(trends_df) >= 2:
        first_count = trends_df.iloc[0]['count']
        last_count = trends_df.iloc[-1]['count']
        if first_count > 0:
            results['growth_rate'] = ((last_count - first_count) / first_count) * 100
"""

    # Execute the trend analysis code
    viz_result = agent.tools["visualize_data"](
        data=trends,
        code=trend_analysis_code
    )
    
    if viz_result.get("success"):
        print(f"✓ Created trend analysis chart: {viz_result['saved_path']}")
        if viz_result.get("data"):
            print(f"Trend analysis results: {json.dumps(viz_result['data'], indent=2)}")
    else:
        print(f"× Error analyzing trends: {viz_result.get('error')}")
    
    
    # 3. Advanced clustering analysis
    print_section("CLUSTERING ANALYSIS")
    
    # Perform clustering
    clusters = agent.tools["cluster_logs"](
        eps=0.5,
        min_samples=2,
        features=["level"]
    )
    
    # Example of LLM-generated code for cluster analysis
    cluster_analysis_code = """
# Analyze clustering results
if 'cluster_summary_df' in locals():
    # Prepare data for visualization
    cluster_summary_df['cluster_name'] = cluster_summary_df['cluster_id'].apply(
        lambda x: 'Noise' if x == -1 else f'Cluster {x}')
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
    
    # Pie chart of cluster distribution
    cluster_summary_df.plot.pie(y='size', labels=cluster_summary_df['cluster_name'], 
                              autopct='%1.1f%%', ax=ax1, startangle=90)
    ax1.set_ylabel('')
    ax1.set_title('Distribution of Logs by Cluster')
    
    # Bar chart of cluster sizes
    non_noise = cluster_summary_df[cluster_summary_df['cluster_id'] != -1].copy()
    if len(non_noise) > 0:
        sns.barplot(x='cluster_name', y='size', data=non_noise, ax=ax2)
        ax2.set_title('Size of Each Cluster (excluding noise)')
        ax2.set_xlabel('Cluster')
        ax2.set_ylabel('Number of Logs')
        ax2.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    
    # Perform deeper analysis if log details are available
    if 'clusters_df' in locals():
        # Analyze what makes each cluster unique
        results['cluster_characteristics'] = {}
        
        for cluster_id in cluster_summary_df['cluster_id'].unique():
            if cluster_id == -1:  # Skip noise
                continue
                
            cluster_logs = clusters_df[clusters_df['cluster_id'] == cluster_id]
            
            # Find most common level in this cluster
            if 'level' in cluster_logs.columns:
                level_counts = cluster_logs['level'].value_counts()
                top_level = level_counts.index[0] if len(level_counts) > 0 else 'N/A'
                level_pct = (level_counts.iloc[0] / len(cluster_logs)) * 100 if len(level_counts) > 0 else 0
                
                results['cluster_characteristics'][f'Cluster {cluster_id}'] = {
                    'size': len(cluster_logs),
                    'dominant_level': top_level,
                    'dominant_level_percentage': level_pct
                }
"""

    # Execute the cluster analysis code
    viz_result = agent.tools["visualize_data"](
        data=clusters,
        code=cluster_analysis_code
    )
    
    if viz_result.get("success"):
        print(f"✓ Created cluster analysis visualization: {viz_result['saved_path']}")
        if viz_result.get("data") and "cluster_characteristics" in viz_result["data"]:
            print("\nCluster characteristics:")
            for cluster, chars in viz_result["data"]["cluster_characteristics"].items():
                print(f"  {cluster}: {chars['size']} logs, {chars['dominant_level']} ({chars['dominant_level_percentage']:.1f}%)")
    else:
        print(f"× Error analyzing clusters: {viz_result.get('error')}")
    
    
    # 4. Custom data exploration
    print_section("CUSTOM DATA EXPLORATION AND ANALYSIS")
    
    # Get logs with response times for performance analysis
    logs = agent.tools["search_logs"](pattern="response_time", limit=50)
    
    # Example of LLM-generated exploratory analysis code
    exploration_code = """
# In-depth analysis of logs with response times
if 'logs_df' in locals():
    # Extract and convert response times to numeric
    if 'response_time' in logs_df.columns:
        try:
            logs_df['response_time_ms'] = pd.to_numeric(logs_df['response_time'], errors='coerce')
        except:
            # Try to extract from string if it's embedded
            import re
            logs_df['response_time_ms'] = logs_df['message'].str.extract(r'response_time=(\d+)').astype(float)
    
    # Check if we have valid response time data
    if 'response_time_ms' in logs_df.columns and not logs_df['response_time_ms'].isna().all():
        response_times = logs_df['response_time_ms'].dropna()
        
        if len(response_times) > 0:
            # Create a figure with multiple performance visualizations
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            
            # Histogram with KDE
            sns.histplot(response_times, kde=True, ax=axes[0, 0])
            axes[0, 0].set_title('Response Time Distribution')
            axes[0, 0].set_xlabel('Response Time (ms)')
            
            # Add mean and median lines
            mean_rt = response_times.mean()
            median_rt = response_times.median()
            axes[0, 0].axvline(mean_rt, color='r', linestyle='--', label=f'Mean: {mean_rt:.2f}ms')
            axes[0, 0].axvline(median_rt, color='g', linestyle='-.', label=f'Median: {median_rt:.2f}ms')
            axes[0, 0].legend()
            
            # Box plot
            sns.boxplot(y=response_times, ax=axes[0, 1])
            axes[0, 1].set_title('Response Time Box Plot')
            axes[0, 1].set_ylabel('Response Time (ms)')
            
            # Time series if timestamp is available
            if 'timestamp' in logs_df.columns:
                try:
                    logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])
                    logs_df = logs_df.sort_values('timestamp')
                    axes[1, 0].plot(logs_df['timestamp'], logs_df['response_time_ms'], marker='o', linestyle='-')
                    axes[1, 0].set_title('Response Time Over Time')
                    axes[1, 0].set_xlabel('Time')
                    axes[1, 0].set_ylabel('Response Time (ms)')
                    axes[1, 0].tick_params(axis='x', rotation=45)
                except:
                    axes[1, 0].text(0.5, 0.5, 'Could not parse timestamps', ha='center', va='center')
            
            # Response time by level if level is available
            if 'level' in logs_df.columns:
                sns.boxplot(x='level', y='response_time_ms', data=logs_df, ax=axes[1, 1])
                axes[1, 1].set_title('Response Time by Log Level')
                axes[1, 1].set_xlabel('Log Level')
                axes[1, 1].set_ylabel('Response Time (ms)')
            
            plt.tight_layout()
            
            # Calculate performance statistics
            results['performance_stats'] = {
                'mean_response_time': float(mean_rt),
                'median_response_time': float(median_rt),
                'min_response_time': float(response_times.min()),
                'max_response_time': float(response_times.max()),
                'p95_response_time': float(response_times.quantile(0.95)),
                'p99_response_time': float(response_times.quantile(0.99)),
                'std_dev': float(response_times.std())
            }
            
            # Identify potential performance issues
            threshold = mean_rt + (2 * response_times.std())
            slow_responses = logs_df[logs_df['response_time_ms'] > threshold]
            
            if len(slow_responses) > 0:
                results['slow_responses'] = {
                    'count': len(slow_responses),
                    'percentage': (len(slow_responses) / len(logs_df)) * 100,
                    'threshold_ms': float(threshold)
                }
"""

    # Execute the exploration code
    viz_result = agent.tools["visualize_data"](
        data=logs,
        code=exploration_code
    )
    
    if viz_result.get("success"):
        print(f"✓ Created performance analysis dashboard: {viz_result['saved_path']}")
        if viz_result.get("data") and "performance_stats" in viz_result["data"]:
            print("\nPerformance statistics:")
            stats = viz_result["data"]["performance_stats"]
            print(f"  Mean: {stats['mean_response_time']:.2f}ms")
            print(f"  Median: {stats['median_response_time']:.2f}ms")
            print(f"  95th percentile: {stats['p95_response_time']:.2f}ms")
            print(f"  99th percentile: {stats['p99_response_time']:.2f}ms")
            
            if "slow_responses" in viz_result["data"]:
                slow = viz_result["data"]["slow_responses"]
                print(f"\nSlow responses (>{slow['threshold_ms']:.2f}ms): {slow['count']} ({slow['percentage']:.1f}%)")
    else:
        print(f"× Error during performance analysis: {viz_result.get('error')}")
    
    
    # 5. Interactive analysis example
    print_section("INTERACTIVE ANALYSIS WITH LLM")
    print("This example demonstrates how the LogAI agent can generate and execute Python analysis code:")
    print("""
Example query: "Analyze response time trends and identify performance outliers. 
Create a visualization showing the relationship between response time and error rates."
    """)
    
    print("\nThe LLM would:")
    print("1. Retrieve the relevant log data using LogAI CLI tools")
    print("2. Generate custom Python code for the specific analysis")
    print("3. Execute the code to produce visualizations and insights")
    print("4. Provide a natural language explanation of the findings")
    
    print("\nFor interactive analysis, run the LogAI agent CLI:")
    print("python3 python/logai_agent_cli.py")

if __name__ == "__main__":
    main() 